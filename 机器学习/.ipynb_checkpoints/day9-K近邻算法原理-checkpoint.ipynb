{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前言\n",
    "$k \\text { 近邻( } k \\text { -Nearest Neighbor, 简称 } k \\mathrm{NN})$ 学习是一种常用的监督学习方法, 其工作机制非常简单: 给定测试样本, 基于某种距离度量找出训练集中与其最 靠近的 $k$ 个训练样本, 然后基于这 $k$ 个“邻居”的信息来进行预测。通常, 在分类任务中可使用“投票法”，即选择这 $k$ 个样本中出现最多的类别标记作为预 测结果; 在回归任务中可使用“平均法”，即将这 $k$ 个样本的实值输出标记的 平均值作为预测结果; 还可基于距离远近进行加权平均或加权投票, 距离越近 的样本权重裁大。\n",
    "\n",
    "影响k近邻算法的两个重要因素：一个是k的取值，二是距离的计算方式。\n",
    "\n",
    "![avatar](../image/机器学习_K近邻算法_图1.jpg)\n",
    "\n",
    "##### k的取值\n",
    "\n",
    "求k的值并不容易。k值小意味着噪声会对结果产生较大的影响, 而k值大则会使计算成本变高。这很大程度上取决于你的实际情况，有些情况下最好是遍历每个可能的k值, 然后自己根据实际来选择K值。\n",
    "\n",
    "##### 距离的计算方式\n",
    "\n",
    "欧氏距离被计算为一个新点和一个现有点在所有输入属性上的差的平方之和的平方根。\n",
    "其他常见的距离度量方法包括:\n",
    "    * Hamming Distance(汉明距离)\n",
    "    * Manhattan Distance(曼哈顿距离)\n",
    "    * Minkowski Distance(闵氏距离)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$k \\text { 近邻}$ 算法在高维情形下会出现数据样本稀疏、距离计算困难等问题，这也被称为\"维数灾难\"。缓解维数灾难的一个重要途径是**降维**。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
