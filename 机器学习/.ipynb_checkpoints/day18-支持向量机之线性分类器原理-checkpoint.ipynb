{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 拉格朗日乘子法和KKT条件\n",
    "\n",
    "拉格朗日乘子法是一种寻找多元函数在一组约束下的极值的方法。\n",
    "\n",
    "参考:\n",
    "1. [深入理解拉格朗日乘子法（Lagrange Multiplier) 和KKT条件](https://www.cnblogs.com/mo-wang/p/4775548.html)\n",
    "2. [Karush-Kuhn-Tucker (KKT)条件\n",
    "Eureka](https://zhuanlan.zhihu.com/p/38163970)\n",
    "\n",
    "#### 等式约束\n",
    "\n",
    "在下等式约束下:\n",
    "\n",
    "$$min_xf(x)$$\n",
    "$$s.t.  h_i(x)=0 (i=1,....m)$$\n",
    "\n",
    "定义拉格朗日函数($\\lambda_i$是拉格朗日乘子):\n",
    "$$L(x,\\lambda)=f(x)+\\Sigma_{i=1}^{m}\\lambda_i g(x)$$\n",
    "\n",
    "对拉格朗日函数求偏导即可求得最优解。\n",
    "\n",
    "#### 非等式约束\n",
    "推广至多个约束等式与约束不等式的情况。考虑标准约束优化问题(或称非线性规划)：\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "\\min & f(\\mathbf{x}) & \\\\\n",
    "\\text { s.t. } & g_{j}(\\mathbf{x})=0, & j=1, \\ldots, m \\\\\n",
    "& h_{k}(\\mathbf{x}) \\leq 0, & k=1, \\ldots, p\n",
    "\\end{array}\n",
    "$$\n",
    "定义拉格朗日函数\n",
    "$$\n",
    "L\\left(\\mathbf{x},\\left\\{\\lambda_{j}\\right\\},\\left\\{\\mu_{k}\\right\\}\\right)=f(\\mathbf{x})+\\sum_{j=1}^{m} \\lambda_{j} g_{j}(\\mathbf{x})+\\sum_{k=1}^{p} \\mu_{k} h_{k}(\\mathbf{x})\n",
    "$$\n",
    "KKT条件是说最优值必须满足以下条件:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_{\\mathbf{x}} L &=\\mathbf{0} \\\\\n",
    "g_{j}(\\mathbf{x}) &=0, \\quad j=1, \\ldots, m \\\\\n",
    "h_{k}(\\mathbf{x}) & \\leq 0 \\\\\n",
    "\\mu_{k} & \\geq 0 \\\\\n",
    "\\mu_{k} h_{k}(\\mathbf{x}) &=0, \\quad k=1, \\ldots, p\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "#### 例子\n",
    "\n",
    "考虑这个问题\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\min & x_{1}^{2}+x_{2}^{2} \\\\\n",
    "\\text { s.t. } & x_{1}+x_{2}=1 \\\\\n",
    "& x_{2} \\leq \\alpha\n",
    "\\end{array}\n",
    "$$\n",
    "其中 $\\left(x_{1}, x_{2}\\right) \\in \\mathbb{R}^{2}, \\quad \\alpha$ 为实数。写出拉格朗日函数\n",
    "$$\n",
    "L\\left(x_{1}, x_{2}, \\lambda, \\mu\\right)=x_{1}^{2}+x_{2}^{2}+\\lambda\\left(1-x_{1}-x_{2}\\right)+\\mu\\left(x_{2}-\\alpha\\right)\n",
    "$$\n",
    "KKT 方程组如下：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial x_{i}} &=0, \\quad i=1,2 \\\\\n",
    "x_{1}+x_{2} &=1 \\\\\n",
    "x_{2}-\\alpha & \\leq 0 \\\\\n",
    "\\mu & \\geq 0 \\\\\n",
    "\\mu\\left(x_{2}-\\alpha\\right) &=0\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 支持向量机思想\n",
    "\n",
    "给定训练样本集$D=\\{(x_1,y_1),(x_2,y_2),...(x_m,y_m)\\}$,$y_i\\in\\{-1,+1\\}$，分类学习最基本的想法就是基于训练集D在样本空间中找到一个划分超平面，将不同类别的样本分开.\n",
    "\n",
    "![avatar](../image/机器学习_支持向量机_图1.jpg)\n",
    "\n",
    "但光是分开是不够的，SVM的核心思想是**尽最大努力使分开的两个类别有最大间隔，这样才使得分隔具有更高的可信度。 而且对于未知的新样本才有很好的分类预测能力（在机器学习中叫泛化能力）**\n",
    "\n",
    "在样本空间中,划分超平面可通过如下线性方程来描述:\n",
    "$$\n",
    "\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}+b=0\n",
    "$$\n",
    "其中 $\\boldsymbol{w}=\\left(w_{1} ; w_{2} ; \\ldots ; w_{d}\\right)$ 为法向量, 决定了超平面的方向; $b$ 为位移项, 决定 了超平面与原点之间的距离。显然，划分超平面可被法向量 $\\boldsymbol{w}$ 和位移 $b$ 确定,\n",
    "下面我们将其记为 (w,b). 样本空间中任意点 $x_i$ 到超平面 (w,b) 的几何距离可定义为:\n",
    "$$\n",
    "\\gamma_i=\\frac{\\left|\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x_i}+b\\right|}{\\|\\boldsymbol{w}\\|}\n",
    "$$\n",
    "\n",
    "假设超平面 $(\\boldsymbol{w}, b)$ 能将训练样本正确分类, 即对于 $\\left(\\boldsymbol{x}_{i}, y_{i}\\right) \\in D,$ 若 $y_{i}=$\n",
    "+1, 则有 $\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b>0 ;$ 若 $y_{i}=-1,$ 则有 $\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b<0 .$ 令\n",
    "$$\n",
    "\\left\\{\\begin{array}{ll}\n",
    "\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b \\geqslant+1, & y_{i}=+1 \\\\\n",
    "\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b \\leqslant-1, & y_{i}=-1\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "如下图所示, 距离超平面最近的这几个训练样本点使上式的等号成立, 它们被称为“支持向量”(support vector), 两个异类支持向量到超平面的距离之和为\n",
    "$$\n",
    "\\gamma=\\frac{2}{\\|\\boldsymbol{w}\\|}\n",
    "$$\n",
    "它被称为“硬间隔” (hard margin).\n",
    "\n",
    "![avatar](../image/机器学习_支持向量机_图2.jpg)\n",
    "\n",
    "SVM为了让模型的泛化性能尽可能好，就需要**最大化间隔**，所以**最大化间隔是SVM的核心问题**。于是，SVM需要解决的问题可以描述为:\n",
    "$$max_{w,b}\\frac{2}{||w||}$$\n",
    "$$y_i(w^Tx+b)\\geq1 ,i=1,2,..m$$\n",
    "等价于:\n",
    "$$min_{w,b}\\frac{||w||^2}{2}$$\n",
    "$$y_i(w^Tx+b)\\geq1 ,i=1,2,..m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 求解支持向量机\n",
    "\n",
    "参考:\n",
    "1.[支持向量机通俗导论](https://www.julyedu.com/question/big/kp_id/23/ques_id/919)\n",
    "2.[机器学习-白板推导支持向量机](https://www.bilibili.com/video/BV1Hs411w7ci?from=search&seid=4578623436593524542)\n",
    "\n",
    "![avatar](../image/机器学习_支持向量机_图3.jpg)\n",
    "\n",
    "我们首先将\n",
    "$$min_{w,b}\\frac{||w||^2}{2}$$\n",
    "$$y_i(w^Tx+b)\\geq1 ,i=1,2,..m$$\n",
    "通过拉格朗日乘子法将上式转为关于(w,b)的无约束问题，上式的拉格朗日函数可写作:\n",
    "$$L(w,b,\\alpha)=\\frac{1}{2}||w||^2+\\Sigma_{i=1}^{m}\\alpha_i(1-y_i(w^Tx_i+b))$$\n",
    "其中$\\boldsymbol{\\alpha}=\\left(\\alpha_{1} ; \\alpha_{2} ; \\ldots ; \\alpha_{m}\\right)$，此时问题转化为:\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\min_{w,b}\\max_{\\alpha} & L(w,b,\\alpha) \\\\\n",
    "\\text { s.t. } & \\alpha_{i} \\geqslant 0\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "在满足**一定条件**时，上述问题又可以转为其强对偶问题:\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\max_{\\alpha}\\min_{w,b} & L(w,b,\\alpha) \\\\\n",
    "\\text { s.t. } & \\alpha_{i} \\geqslant 0\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "先求解$\\min_{w,b} L(w,b,\\alpha)$，此时$\\alpha$看成常数，对其中的$w$和$b$求偏导即可得:\n",
    "$$w=\\Sigma_{i=1}^{m}a_iy_ix_i$$\n",
    "$$0=\\Sigma_{i=1}^{m}a_iy_i$$\n",
    "将上面两个式子带回拉格朗日函数有:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(w, b, \\alpha) \n",
    "&=\\frac{1}{2}w^Tw+\\Sigma_{i=1}^{m}\\alpha_i(1-y_i(w^Tx_i+b)) \\\\\n",
    "&=\\frac{1}{2} \\sum_{i, j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} x_{i}^{T} x_{j}-\\sum_{i, j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} x_{i}^{T} x_{j}-b \\sum_{i=1}^{n} \\alpha_{i} y_{i}+\\sum_{i=1}^{n} \\alpha \\\\\n",
    "&=\\sum_{i=1}^{n} \\alpha_{i}-\\frac{1}{2} \\sum_{i, j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} x_{i}^{T} x_{j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "所以最终问题转化为:\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\max_{\\alpha} & L(\\alpha)=\\sum_{i=1}^{n} \\alpha_{i}-\\frac{1}{2} \\sum_{i, j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} x_{i}^{T} x_{j}\\\\\n",
    "\\text { s.t. } & \\alpha_{i} \\geqslant 0\\\\\n",
    "& \\Sigma_{i=1}^{m}a_iy_i=0 \\\\\n",
    "\\end{array} \\\\\n",
    "$$\n",
    "\n",
    "由于求解SVM是凸二次优化问题，必然其对偶问题一定是强对偶的，在强对偶时，又必然满足KKT条件，我们可以通过KKT条件求解部分最优值，KKT条件如下:\n",
    "$$\n",
    "\\left\\{\\begin{array}{l}\n",
    "\\alpha_{i} \\geqslant 0 \\\\\n",
    "y_i(w^Tx+b)-1 \\geqslant 0 \\\\\n",
    "\\alpha_{i}\\left(y_i(w^Tx+b)-1\\right)=0\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "综上，假设最终得到的最优解$w^{*}$，其形式如下:\n",
    "$$\n",
    "w^{*}=\\Sigma_{i=1}^{m}a_iy_ix_i\n",
    "$$\n",
    "从上式可知$w^{*}$其实是学习样本的线性组合,同时根据$\\alpha_{i}\\left(y_i(w^Tx+b)-1\\right)=0$可知,**仅在支持向量上，$\\alpha_{i}$才可能非零**,假设支持向量为$(x_k,y_k)$,且$\\left(y_k(w^Tx_k+b)-1\\right)=0$,此时可以通过支持向量求解出$b^{*}$,即:\n",
    "$$b^{*}=y_k-\\Sigma_{i=1}^{m}a_iy_ix^{T}_ix_k$$\n",
    "\n",
    "总结:\n",
    "$$w^{*}=\\Sigma_{i=1}^{m}a_iy_ix_i$$\n",
    "$$b^{*}=y_k-\\Sigma_{i=1}^{m}a_iy_ix^{T}_ix_k$$\n",
    "由于$w^{*}$和$b^{*}$都和$\\alpha$有关，从这可以得出支持向量机的一个重要性质:训练完成后,大部分的训练样本都不需保留，**最终模型仅与支持向量有关.**\n",
    "\n",
    "对于\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\max_{\\alpha} & L(\\alpha)=\\sum_{i=1}^{n} \\alpha_{i}-\\frac{1}{2} \\sum_{i, j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} x_{i}^{T} x_{j}\\\\\n",
    "\\text { s.t. } & \\alpha_{i} \\geqslant 0\\\\\n",
    "& \\Sigma_{i=1}^{m}a_iy_i=0 \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "上式是一个二次规划问题，人们通过SMO算法来求解上式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 软间隔\n",
    "\n",
    "前面介绍的支持向量机要求所有样本均满足约束，即所有样本必须划分正确，这称为“软间隔”，而软间隔则是允许某些样本不满足约束:\n",
    "$$y_{i}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b\\right) < 1$$\n",
    "\n",
    "当然，不满足约束的样本应尽可能少，于是优化目标可写为:\n",
    "$$\n",
    "\\min _{\\boldsymbol{w}, b} \\frac{1}{2}\\|\\boldsymbol{w}\\|^{2}+C*Loss(z)\n",
    "$$\n",
    "\n",
    "其中 $C>0$ 是一个常数, $Loss(z)$可以定义为:\n",
    "$$Loss(z)=\\sum^{N}_{i=1}I\\{z<1\\}$$\n",
    "$$其中z=y_{i}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b\\right)$$\n",
    "其中$I\\{...\\}$代表不符合约束的数目.\n",
    "\n",
    "常用的损失函数有:\n",
    "* 0/1损失函数\n",
    "$$\\ell_{0 / 1}(z)=\\left\\{\\begin{array}{ll}\n",
    "1, & \\text { if } z<0 \\\\\n",
    "0, & \\text { otherwise }\n",
    "\\end{array}\\right.$$\n",
    "由于0/1损失函数非凸、非连续，数学性质不太好，所以常用其他替代损失函数。\n",
    "* hinge损失:\n",
    "$$\n",
    "\\ell_{\\text {hinge}}(z)=\\max (0,1-z)\n",
    "$$\n",
    "hinge损失具有较好的数学性质，比较常用.\n",
    "\n",
    "若采用hinge损失，则损失变成:\n",
    "$$\n",
    "\\min _{\\boldsymbol{w}, b} \\frac{1}{2}\\|\\boldsymbol{w}\\|^{2}+C \\sum_{i=1}^{m} \\max \\left(0,1-y_{i}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b\\right)\\right)\n",
    "$$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\text { s.t. } \\quad y_{i}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b\\right) \\geqslant 1-\\xi_{i}\\\\\n",
    "&\\xi_{i} \\geqslant 0, i=1,2, \\ldots, m\n",
    "\\end{aligned}\n",
    "$$\n",
    "这就是常用的“软间隔支持向量机”.\n",
    "\n",
    "通过拉格朗日乘子法得到其拉格朗日函数为:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\boldsymbol{w}, b, \\boldsymbol{\\alpha}, \\boldsymbol{\\xi}, \\boldsymbol{\\mu})=& \\frac{1}{2}\\|\\boldsymbol{w}\\|^{2}+C \\sum_{i=1}^{m} \\xi_{i} \\\\\n",
    "&+\\sum_{i=1}^{m} \\alpha_{i}\\left(1-\\xi_{i}-y_{i}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b\\right)\\right)-\\sum_{i=1}^{m} \\mu_{i} \\xi_{i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "其中 $\\alpha_{i} \\geqslant 0, \\mu_{i} \\geqslant 0$ 是拉格朗日乘子.\n",
    "\n",
    "强对偶问题:\n",
    "\\begin{array}{ll}\n",
    "\\max_{\\alpha,\\mu}\\min_{w,b,\\boldsymbol{\\xi}} & L(w,b,\\boldsymbol{\\xi},\\alpha,\\mu) \\\\\n",
    "\\text { s.t. } & \\alpha_{i} \\geqslant 0\\\\\n",
    "& \\mu_{i} \\geqslant 0 \\\\\n",
    "\\end{array} \n",
    "对$w,b,\\boldsymbol{\\xi}$的偏导为零可得\n",
    "$$w=\\Sigma_{i=1}^{m}a_iy_ix_i$$\n",
    "$$0=\\Sigma_{i=1}^{m}a_iy_i$$\n",
    "$$C=\\alpha_{i}+\\mu_{i}$$\n",
    "将上式带回其对偶问题有:\n",
    "\\begin{array}{ll}\n",
    "\\max_{\\alpha} & L(\\alpha)=\\sum_{i=1}^{n} \\alpha_{i}-\\frac{1}{2} \\sum_{i, j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} x_{i}^{T} x_{j}\\\\\n",
    "\\text { s.t. } & 0 \\le \\alpha_{i} \\le C \\\\\n",
    "& \\Sigma_{i=1}^{m}a_iy_i=0 \\\\\n",
    "\\end{array}\n",
    "其中$\\alpha_{i} \\le C$是由于$C=\\alpha_{i}+\\mu_{i}$，且$\\alpha_i,\\mu_i$都大于等于0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
