{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 贝叶斯决策论\n",
    "\n",
    "贝叶斯决策论(Bayesian decision theory)是概率框架下实施决策的基本方法.对分类任务来说, 在所有相关概率都已知的理想情形下，贝叶斯决策论考虑如何基于这些概率和误判损失来选择最优的类别标记. 下面我们以多分类任务\n",
    "为例来解释其基本原理。\n",
    "\n",
    "假设有N种可能的类别标记，即 $\\mathcal{Y}=\\left\\{c_{1}, c_{2}, \\ldots, c_{N}\\right\\}, \\lambda_{i j}$ 是将一个真实\n",
    "标记为 $c_{j}$ 的样本误分类为 $c_{i}$ 所产生的损失. 基于后验概率 $P\\left(c_{i} \\mid \\boldsymbol{x}\\right)$ 可获得将 样本 $\\boldsymbol{x}$ 分类为 $c_{i}$ 所产生的期望损失(expected loss), 即在样本$\\boldsymbol{x}$上的“条件风\n",
    "险” (conditional risk)\n",
    "$$\n",
    "R\\left(c_{i} \\mid \\boldsymbol{x}\\right)=\\sum_{j=1}^{N} \\lambda_{i j} P\\left(c_{j} \\mid \\boldsymbol{x}\\right)\n",
    "$$\n",
    "我们的任务是寻找一个判定准则 $h: \\mathcal{X} \\mapsto \\mathcal{Y}$ 以最小化总体风险\n",
    "$$\n",
    "R(h)=\\mathbb{E}_{\\boldsymbol{x}}[R(h(\\boldsymbol{x}) \\mid \\boldsymbol{x})]\n",
    "$$\n",
    "显然, 对每个样本 $\\boldsymbol{x},$ 若 $h$ 能最小化条件风险 $R(h(\\boldsymbol{x}) \\mid \\boldsymbol{x}),$ 则总体风险 $R(h)$ 也 将被最小化. 这就产生了贝叶斯判定准则(Bayes decision rule)：为最小化总体\n",
    "风险, 只需在每个样本上选择那个能使条件风险 $R(c \\mid \\boldsymbol{x})$ 最小的类别标记, 即\n",
    "$$\n",
    "h^{*}(\\boldsymbol{x})=\\underset{c \\in \\mathcal{Y}}{\\arg \\min } R(c \\mid \\boldsymbol{x})\n",
    "$$\n",
    "此时, $h^{*}$ 称为贝叶斯最优分类器(Bayes optimal classifier), 与之对应的总体风 险 $R\\left(h^{*}\\right)$ 称为贝叶斯风险(Bayes risk). $1-R\\left(h^{*}\\right)$ 反映了分类器所能达到的最好性能，即通过机器学习所能产生的模型精度的理论上限。\n",
    "具体来说，若目标是最小化分类错误率, 则误判损失 $\\lambda_{i j}$ 可写为\n",
    "$$\n",
    "\\lambda_{i j}=\\left\\{\\begin{array}{ll}\n",
    "0, & \\text { if } i=j \\\\\n",
    "1, & \\text { otherwise }\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "此时条件风险\n",
    "$$\n",
    "R(c \\mid \\boldsymbol{x})=1-P(c \\mid \\boldsymbol{x})\n",
    "$$\n",
    "于是, 最小化分类错误率的贝叶斯最优分类器为\n",
    "$$\n",
    "h^{*}(\\boldsymbol{x})=\\underset{c \\in \\mathcal{Y}}{\\arg \\max } P(c \\mid \\boldsymbol{x})\n",
    "$$\n",
    "即对每个样本 $\\boldsymbol{x},$ 选择能使后验概率 $P(c \\mid \\boldsymbol{x})$ 最大的类别标记.\n",
    "不难看出, 欲使用贝叶斯判定准则来最小化决策风险，首先要获得后验概\n",
    "率 P $(c \\mid \\boldsymbol{x})$. \n",
    "\n",
    "然而, 在现实任务中这通常难以直接获得. 从这个角度来看, 机\n",
    "器学习所要实现的是基于有限的训练样本集尽可能准确地估计出后验概率\n",
    "$P(c \\mid \\boldsymbol{x}) .$ 大体来说, 主要有两种策略: 给定 $\\boldsymbol{x},$ 可通过直接建模 $P(c \\mid \\boldsymbol{x})$ 来 预测 c, 这样得到的是“判别式模型” (discriminative models); 也可先对联合概率分布 $P(\\boldsymbol{x}, c)$ 建模, 然后再由此获得 $P(c \\mid \\boldsymbol{x}),$ 这样得到的是“生成式模型”(generative models).决策树、BP神经网络、支持向量机等, 都可归入判别式模型的范畴. 对生成式模型来说, 必然考虑\n",
    "$$\n",
    "P(c \\mid \\boldsymbol{x})=\\frac{P(\\boldsymbol{x}, c)}{P(\\boldsymbol{x})}\n",
    "$$\n",
    "基于贝叶斯定理, $P(c \\mid \\boldsymbol{x})$ 可写为\n",
    "$$\n",
    "P(c \\mid \\boldsymbol{x})=\\frac{P(c) P(\\boldsymbol{x} \\mid c)}{P(\\boldsymbol{x})}\n",
    "$$\n",
    "其中, $P(c)$ 是类“先验”(prior)概率; $P(\\boldsymbol{x} \\mid c)$ 是样本 $\\boldsymbol{x}$ 相对于类标记 $c$ 的类 条件概率(class-conditional probability), 或称为“似然”(likelihood); P( $x$ ) 是 用于归一化的“证据” (evidence)因子. 对给定样本 $\\boldsymbol{x},$ 证据因子 $P(\\boldsymbol{x})$ 与类标\n",
    "均 记无关, 因此估计 $P(c \\mid \\boldsymbol{x})$ 的问题就转化为如何基于训练数据 $D$ 来估计先验\n",
    "$P(c)$ 和似然 $P(\\boldsymbol{x} \\mid c)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 朴素贝叶斯分类器\n",
    "\n",
    "朴素贝叶斯分类器采用了“属性条件独立性假设\":对已知类别,假设所有属性相互独立.换言之，假设每个属性独立地对分类结果发生影响.\n",
    "\n",
    "基于**属性条件独立假设**，$P(c \\mid \\boldsymbol{x})$可重写为:\n",
    "$$\n",
    "P(c \\mid \\boldsymbol{x})=\\frac{P(c) P(\\boldsymbol{x} \\mid c)}{P(\\boldsymbol{x})}=\\frac{P(c)}{P(\\boldsymbol{x})} \\prod_{i=1}^{d} P\\left(x_{i} \\mid c\\right)\n",
    "$$\n",
    "其中 $d$ 为属性数目, **$x_{i}$ 为 $\\boldsymbol{x}$在第$i$个属性上的取值**.\n",
    "由于对所有类别来说 $P(\\boldsymbol{x})$ 相同, 因此基于式 (7.6) 的贝叶斯判定准则有\n",
    "$$\n",
    "h_{n b}(\\boldsymbol{x})=\\underset{c \\in \\mathcal{Y}}{\\arg \\max } P(c) \\prod_{i=1}^{d} P\\left(x_{i} \\mid c\\right)\n",
    "$$\n",
    "这就是朴素贝叶斯分类器的表达式.\n",
    "显然, 朴素贝叶斯分类器的训练过程就是基于训练集 D 来估计类先验概率\n",
    "$P(c),$ 并为每个属性估计条件概率 $P\\left(x_{i} \\mid c\\right)$\n",
    "令 $D_{c}$ 表示训练集 $D$ 中第 $c$ 类样本组成的集合, 若有充足的独立同分布样\n",
    "本, 则可容易地估计出类先验概率\n",
    "$$\n",
    "P(c)=\\frac{\\left|D_{c}\\right|}{|D|}\n",
    "$$\n",
    "对离散属性而言, 令 $D_{c, x_{i}}$ 表示 $D_{c}$ 中在第 $i$ 个属性上取值为 $x_{i}$ 的样本组成的 集合, 则条件概率 $P\\left(x_{i} \\mid c\\right)$ 可估计为\n",
    "$$\n",
    "P\\left(x_{i} \\mid c\\right)=\\frac{\\left|D_{c, x_{i}}\\right|}{\\left|D_{c}\\right|}\n",
    "$$\n",
    "对连续属性可考虑概率密度函数, 假定 $p\\left(x_{i} \\mid c\\right) \\sim \\mathcal{N}\\left(\\mu_{c, i}, \\sigma_{c, i}^{2}\\right),$ 其中 $\\mu_{c, i}$ 和 $\\sigma_{c, i}^{2}$\n",
    "分别是第 $c$ 类样本在第 $i$ 个属性上取值的均值和方差, 则有\n",
    "$$\n",
    "p\\left(x_{i} \\mid c\\right)=\\frac{1}{\\sqrt{2 \\pi} \\sigma_{c, i}} \\exp \\left(-\\frac{\\left(x_{i}-\\mu_{c, i}\\right)^{2}}{2 \\sigma_{c, i}^{2}}\\right)\n",
    "$$\n",
    "\n",
    "为了避免其他属性携带的信息被训练集中未出现的属性值“抹去”, 在估计概率值时通常要进行“平滑”，常用“拉普拉斯修正”. 具体来说，令N表示训练集D中可能的类别数, $N_{i}$ 表示第$i$个属性可能的取值数,则\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{P}(c) &=\\frac{\\left|D_{c}\\right|+1}{|D|+N} \\\\\n",
    "\\hat{P}\\left(x_{i} \\mid c\\right) &=\\frac{\\left|D_{c, x_{i}}\\right|+1}{\\left|D_{c}\\right|+N_{i}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "具体例子可参考周志华《机器学习》p151-154.\n",
    "\n",
    "通过上述推导，我们可以知道朴素贝叶斯分类器中的两个基本假设:\n",
    "1. 每个特征相互独立\n",
    "2. 每个特征同等重要\n",
    "\n",
    "在实际中，上述两个假设大部分是不成立的，尽管上述假设存在一些小的瑕疵，但朴素贝叶斯的实际效果却很好。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
