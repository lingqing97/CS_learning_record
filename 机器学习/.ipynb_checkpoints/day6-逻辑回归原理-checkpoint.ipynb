{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前导知识"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 极大似然估计\n",
    "\n",
    "极大似然估计(Maximum Likelihood Estimation，简称MLE)，是根据数据采样来估计概率分布参数的经典方法。\n",
    "\n",
    "令$D_c$表示训练集D中第c类样本组成的集合，假设这些样本是**独立同分布的**，则参数$\\theta_c$对于数据集$D_c$的似然是:\n",
    "$$P(D_c|\\theta_c)=\\Pi_{x\\in D_c}P(x|\\theta_c)$$\n",
    "对$\\theta_c$进行极大似然估计，就是去寻找能最大化似然$P(D_c|\\theta_c)$的参数值$\\hat\\theta_c$。\n",
    "**直观上看，极大似然估计是试图在$\\theta_c$所有可能的取值中，找到一个能使数据出现的\"可能性\"最大的值。**\n",
    "为了求解极大似然，通常使用对数似然:\n",
    "$$LL(\\theta_c)=log P(D_c|\\theta_c)$$\n",
    "$$=\\Sigma_{x\\in D_c} log P(x|\\theta_c)$$\n",
    "此时参数$\\theta_c$的极大似然估计$\\hat\\theta_c$为:\n",
    "$$\\hat\\theta_c=argmax_{\\theta_c}LL(\\theta_c)$$\n",
    "\n",
    "**极大似然估计的估计结果准确性严重依赖于所假设的概率分布形式是否符合潜在的真实数据分布。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 梯度下降法\n",
    "\n",
    "梯度下降法(gradient descent)是一种常用的一阶优化方法，是求解无约束优化问题最简单、经典的方法之一。\n",
    "\n",
    "考虑无约束优化问题$min_xf(x)$,其中$f(x)$为连续可微积函数.若能构造一个序列$x^0,x^1,x^2,...$满足\n",
    "$$f(x^{t+1})<f(x^t),t=0,1,2...$$\n",
    "则不断执行该过程即可收敛到局部极小点，根据泰勒展式有\n",
    "$$f(\\boldsymbol{x}+\\Delta \\boldsymbol{x}) \\simeq f(\\boldsymbol{x})+\\Delta \\boldsymbol{x}^{\\mathrm{T}} \\nabla f(\\boldsymbol{x})$$\n",
    "于是，欲满足$f(\\boldsymbol{x}+\\Delta \\boldsymbol{x})<f(\\boldsymbol{x})$,可选择\n",
    "$$\\Delta \\boldsymbol{x}=-\\gamma \\nabla f(\\boldsymbol{x})$$\n",
    "其中步长$\\gamma$是一个小常数。这就是梯度下降法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逻辑回归原理\n",
    "\n",
    "在线性回归中我们讨论了如何使用线性模型用于回归学习，此外，借助广义的线性模型即$y=g^{-1}(w^Tx+b)$也可以用于分类任务：需要找到一个函数将分类任务的真实标记与线性回归模型的预测值联系起来。\n",
    "![avatar](../image/机器学习_逻辑回归_对数几率函数.jpg)\n",
    "对数几率函数(logisttic function)就可以作为这样的一个映射函数,其形式为:\n",
    "$$y=\\frac{1}{1+e^{-z}}$$\n",
    "对数几率函数将预测值z转化为一个概率值y，当z大于0时可以判为正例，小于0可以判为负例。\n",
    "将线性回归带入对数几率:\n",
    "$$y=\\frac{1}{1+e^{-(w^Tx+b)}}$$\n",
    "上式可转换为:\n",
    "$$ln\\frac{y}{1-y}=w^Tx+b$$\n",
    "这里的概率值y视为样本x作为正例的可能性，1-y是其为反例的可能性。\n",
    "上述模型即逻辑回归，更准确地讲应该是`对数几率回归`。注意，它的名字虽然是“回归”，但实际是一种分类学习方法。\n",
    "逻辑回归预测出的是**类别的近似概率**，这对许多需利用概率辅助决策的任务很有用。\n",
    "\n",
    "这里将y视为类后验概率估计$p(y=1|x)$，则逻辑回归可写为:\n",
    "$$ln\\frac{p(y=1|x)}{p(y=0|x)}=w^Tx+b$$\n",
    "由$p(y=0|x)=1-p(y=1|x)$可以得到:\n",
    "$$p(y=1|x)=\\frac{e^{w^Tx+b}}{1+e^{w^Tx+b}}$$\n",
    "$$p(y=0|x)=\\frac{1}{1+e^{w^Tx+b}}$$\n",
    "\n",
    "为了便于讨论，令$\\beta=(w^T;b),\\hat{x}=(x;1)$，则$w^Tx+b=\\beta^{T}\\hat{x}$,同时令$p(y=1|x)=h_{\\beta}(x)$,即:\n",
    "$$p(y=1|x)=h_{\\beta}(x)=\\frac{e^{\\beta^T\\hat{x}}}{1+e^{\\beta^T\\hat{x}}}$$\n",
    "$$p(y=0|x)=1-h_{\\beta}(x)=\\frac{e^{\\beta^T\\hat{x}}}{1+e^{\\beta^T\\hat{x}}}$$\n",
    "我们可以将上述两个概率公式合二为一:\n",
    "$$p(y_i|x_i;\\beta)=Cost(h_{\\beta}(x),y)=h_{\\beta}(x)^y(1-h_{\\beta}(x))^{1-y}$$\n",
    "上述函数，我们称之为**代价函数**。\n",
    "\n",
    "之后，我么可通过“极大似然估计法”来估计参数$\\beta$,给定数据集$\\{(x_i,y_i)\\}^{m}_{i=1}$，其对数似然为:\n",
    "$$LL(\\beta)=\\Sigma^{m}_{i=1} lnp(y_i|x_i;\\beta)$$\n",
    "最大化$LL(\\beta)$等价于最小化$-LL(\\beta)$,即:\n",
    "$$J(\\beta)=\\Sigma^{m}_{i=1} (-y_ilnh_{\\beta}(x_i)+(y_i-1)ln(1-h_{\\beta}(x_i)))$$\n",
    "上式是关于$\\beta$的高阶可导连续凸函数，根据凸函数理论，可以通过经典的数值优化算法如梯度下降法、牛顿法等都可求解其最优解，于是可得:\n",
    "$$\\beta^{*}=argmin_{\\beta}J(\\beta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 梯度下降法求解逻辑回归的最优值\n",
    "参考:[周志华《机器学习》学习笔记——Logistic回归](https://blog.csdn.net/liumingpei/article/details/78244323)\n",
    "\n",
    "\n",
    "现在我们对$J(\\beta)$求关于$\\beta$的偏导。\n",
    "\n",
    "由链式法则有:\n",
    "$$\\frac{\\partial J(\\beta)}{\\partial \\beta}=\\frac{\\partial J(\\beta)}{\\partial h_{\\beta}(x)}*\\frac{\\partial h_{\\beta}(x)}{\\partial \\beta^Tx}*\\frac{\n",
    "\\partial \\beta^Tx}{\\partial \\beta}$$\n",
    "\n",
    "其中第一项:\n",
    "$$\\frac{\\partial J(\\beta)}{\\partial h_{\\beta}(x)}=-y*\\frac{1}{h_{\\beta}(x)}+(y-1)\\frac{1}{1-h_{\\beta}(x)}$$\n",
    "\n",
    "第二项对数几率函数求导为:\n",
    "$$\\frac{\\partial h_{\\beta}(x)}{\\partial \\beta^Tx}=h_{\\beta}(x)(1-h_{\\beta}(x))$$\n",
    "\n",
    "最后一项:\n",
    "$$\\frac{\n",
    "\\partial \\beta^Tx}{\\partial \\beta}=x$$\n",
    "\n",
    "因此，梯度下降的迭代式为:\n",
    "$$\\beta \\leftarrow \\beta-\\gamma \\triangledown J(\\beta)$$\n",
    "$$\\triangledown J(\\beta)=(y-h_{\\beta}(x))*x$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
