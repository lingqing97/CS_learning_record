{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先定义一个含单隐藏层的多层感知机。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=3, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net=nn.Sequential(nn.Linear(4,3),nn.ReLU(),nn.Linear(3,1)) #pytorch已进行默认的初始化\n",
    "\n",
    "print(net)\n",
    "X=torch.rand(2,4)\n",
    "Y=net(X).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 访问模型参数\n",
    "\n",
    "对于Sequential类，我们可以通过Module类的parameters()或者named_parameters()方法来访问所有参数(以迭代器的形式返回)，后者除了返回参数Tensor外还会返回其名字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n",
      "0.weight torch.Size([3, 4])\n",
      "0.bias torch.Size([3])\n",
      "2.weight torch.Size([1, 3])\n",
      "2.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "print(type(net.named_parameters()))\n",
    "#返回的名字自动加上了层数的索引作为前缀\n",
    "for name,param in net.named_parameters():\n",
    "    print(name,param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 初始化模型参数\n",
    "\n",
    "PyTorch的init模块里提供了多种预设的初始化方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight tensor([[-0.0200,  0.0166,  0.0123, -0.0084],\n",
      "        [-0.0058, -0.0201,  0.0060,  0.0103],\n",
      "        [ 0.0028, -0.0147,  0.0003,  0.0023]])\n",
      "2.weight tensor([[-0.0015,  0.0107, -0.0010]])\n"
     ]
    }
   ],
   "source": [
    "#将权重参数初始化成均值为0、标准差为0.01的正态分布随机数，并依然将偏差参数清零\n",
    "\n",
    "for name,param in net.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        init.normal_(param,mean=0,std=0.01)\n",
    "        print(name,param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.bias tensor([0., 0., 0.])\n",
      "2.bias tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "#下面使用常数来初始化偏差参数\n",
    "\n",
    "for name,param in net.named_parameters():\n",
    "    if 'bias' in name:\n",
    "        init.constant_(param,val=0)\n",
    "        print(name,param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 自定义初始化方法\n",
    "\n",
    "在下面的例子里，我们令权重有一半概率初始化为0，有另一半概率初始化为[-10,-5]和[5,10]两个区间里均匀分布的随机数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight tensor([[-5.6535, -6.5999, -0.0000, -6.8306],\n",
      "        [-0.0000,  8.4150,  0.0000, -0.0000],\n",
      "        [ 0.0000, -0.0000,  5.4649,  0.0000]])\n",
      "2.weight tensor([[8.1392, 5.2721, -0.0000]])\n"
     ]
    }
   ],
   "source": [
    "def init_weight_(tensor):\n",
    "    #这里使用一个inplace改变Tensor值得函数，而且这个过程是不记录梯度的\n",
    "    with torch.no_grad():\n",
    "        tensor.uniform_(-10,10)\n",
    "        tensor*=(tensor.abs()>=5).float()\n",
    "\n",
    "\n",
    "for name,param in net.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        init_weight_(param)\n",
    "        print(name,param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还可以通过改变这些参数data来改写模型参数值同时不会影响梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.bias tensor([1., 1., 1.])\n",
      "2.bias tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "for name,param in net.named_parameters():\n",
    "    if 'bias' in name:\n",
    "        param.data+=1\n",
    "        print(name,param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 共享模型参数\n",
    "\n",
    "在有些情况下，我们希望在多个层之间共享模型参数。\n",
    "\n",
    "Module类的forward函数里多次调用同一个层可以共享模型参数，此外，如果我们传入Sequential的模块是同一个Module实例的话参数也是共享的。\n",
    "\n",
    "在内存中这些层其实是一个对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=1, out_features=1, bias=False)\n",
      "  (1): Linear(in_features=1, out_features=1, bias=False)\n",
      ")\n",
      "0.weight tensor([[3.]])\n"
     ]
    }
   ],
   "source": [
    "linear=nn.Linear(1,1,bias=False)\n",
    "net=nn.Sequential(linear,linear)\n",
    "print(net)\n",
    "#这里只有一个权重参数\n",
    "for name,param in net.named_parameters():\n",
    "    init.constant_(param,val=3)\n",
    "    print(name,param.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
