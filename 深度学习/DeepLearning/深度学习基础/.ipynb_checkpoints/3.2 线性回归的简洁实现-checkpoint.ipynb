{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性回归的简洁实现\n",
    "\n",
    "在本节，我们将使用PyTorch更方便地实现线性回归的训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "num_inputs=2\n",
    "num_examples=1000\n",
    "true_w=[2,-3]\n",
    "true_b=4.2\n",
    "features=torch.tensor(np.random.normal(0,1,(num_examples,num_inputs)),dtype=torch.float32)\n",
    "labels=true_w[0]*features[:,0]+true_w[1]*features[:,1]+true_b\n",
    "labels+=torch.tensor(np.random.normal(0,0.01,size=labels.size()),dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读取数据\n",
    "\n",
    "PyTorch提供了data包来读取数据。由于data常用作变量名，我们将导入的data模块用Data代替。在每一次迭代中，我们将随机读取包含10个数据样本的小批量。\n",
    "\n",
    "函数说明\n",
    "\n",
    "class torch.utils.data.TensorDataset(data_tensor,target_tensor)\n",
    "\n",
    "包装训练数据和标签的数据集\n",
    "\n",
    "参数:\n",
    "* data_tensor(Tensor):样本数据\n",
    "* target_tensor(Tensor):标签\n",
    "\n",
    "class torch.utils.data.Dataloader(dataset,batch_size=1,shuffle=False,..)\n",
    "\n",
    "数据加载器，在数据集上提供单进程或多进程迭代器\n",
    "\n",
    "参数:\n",
    "* dataset(Dataset):加载数据的数据集\n",
    "* batch_size(int):batch_Size\n",
    "* shuffle(bool):设置为Ture时会在每个epoch重新打乱数据(默认:False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "\n",
    "batch_size=10\n",
    "#将训练数据的特征和标签组合\n",
    "dataset=Data.TensorDataset(features,labels)\n",
    "#随机读取小批量\n",
    "#这里的data_iter也是一个迭代器\n",
    "data_iter=Data.DataLoader(dataset,batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8110, -0.1360],\n",
      "        [ 0.9777, -0.3895],\n",
      "        [-0.3851, -2.0484],\n",
      "        [-1.2841,  2.3703],\n",
      "        [ 0.9958, -1.9146],\n",
      "        [-1.4517,  0.0620],\n",
      "        [-0.5573,  0.0842],\n",
      "        [ 0.8103,  0.8467],\n",
      "        [ 0.6521,  0.3122],\n",
      "        [-0.4478, -0.3239]]) tensor([ 2.9930,  7.3226,  9.5793, -5.4694, 11.9473,  1.1055,  2.8418,  3.2882,\n",
      "         4.5754,  4.2692])\n"
     ]
    }
   ],
   "source": [
    "for X,y in data_iter:\n",
    "    print(X,y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义模型\n",
    "\n",
    "PyTorch提供了大量预定义的层。\n",
    "\n",
    "首先导入torch.nn模块。实际上,'nn'是neural networks(神经网络)的缩写。该模型定义了大量神经网络的层。nn的核心数据结构是Module,它是一个抽象概念。在实际使用中，最常用的做法是继承nn.Module,撰写自己的网络/层。\n",
    "\n",
    "**一个nn.Module实例应该包含一些层以及返回输出的前向传播方法。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    def __init__(self,n_feature):\n",
    "        super(LinearNet,self).__init__()\n",
    "        self.linear=nn.Linear(n_feature,1)\n",
    "    #forward定义前向传播\n",
    "    def forward(self,x):\n",
    "        y=self.linear(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearNet(\n",
      "  (linear): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net=LinearNet(num_inputs)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事实上我们还可以用nn.Sequential来更加方便搭建网络,Sequential是一个有序的容器，网络层将按照在传入Sequential的顺序依次被添加到计算图中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n",
      "Sequential(\n",
      "  (linear): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#写法一\n",
    "net1=nn.Sequential(\n",
    "    nn.Linear(num_inputs,1)\n",
    "    #此处还可以传入其他层\n",
    ")\n",
    "#写法二\n",
    "net2=nn.Sequential()\n",
    "net2.add_module('linear',nn.Linear(num_inputs,1))\n",
    "#net.add_module...\n",
    "\n",
    "print(net1)\n",
    "print(net2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以通过net.parameters()来查看模型所有的可学习参数，此函数将返回一个生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.4893, -0.1494]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.2035], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in net.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性回归输出层中的神经元和输入层中各个输入完全连接。因此，线性回归的输出层又叫全连接层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 初始化模型参数\n",
    "\n",
    "在使用net前，我么需要初始化模型参数。PyTorch在init模块中提供了多种参数初始化方法。这里的init是initializer的缩写形式。我们通过init.normal_将权重参数每个元素初始化为随机采样于均值为0，标准差为0.01的正态分布。偏差会初始化为零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.], requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import init\n",
    "\n",
    "init.normal_(net1[0].weight,mean=0,std=0.01)\n",
    "init.constant_(net1[0].bias,val=0)#也可以直接修改bias的data:net[0].bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意:如果这里的net是继承与nn.module的LinearNet,那么上面的代码会报错,net[0].weight应改为net.linear.weight,bias亦然。因为net[0]这样根据下标访问子模块的写法只有当net是个ModuleList或者Sequential实例时才可以**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义损失函数\n",
    "\n",
    "PyTorch在nn模块中提供了各种损失函数，这些损失函数可看作是一种特殊的层，PyTorch也将这些损失函数实现为nn.Module的子类。我们现在使用它提供的均方误差损失作为模型的损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义优化算法\n",
    "\n",
    "同样，我么也无需自己实现小批量随机梯度下降算法。torch.optim模块提供了很多常用的优化算法比如SGD、Adam和RMSProp等。下面我们创建一个用于优化net所有参赛的优化器实例，并制定学习率为0.03的小批量随机梯度下降(SGD)为优化算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.03\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer=optim.SGD(net1.parameters(),lr=0.03)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还可以为不同子网络设置不同的学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=optim.SGD([\n",
    "    {'params':net.subnet1.parameters()},#lr=0.03\n",
    "    {'params':net.subnet2.parameters(),'lr':0.01}\n",
    "],lr=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调整学习率有两种做法:\n",
    "* 修改optimizer.param_groups中对应的学习率\n",
    "* 新建优化器。但对于使用的动量的优化器(如Adam)，会丢失动量等状态信息，可能会造成损失函数的收敛出现震荡等情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#调整学习率\n",
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr']*=0.1  #学习率为之前的0.1倍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练模型\n",
    "\n",
    "在训练模型时，我们通过调用optim实例的step函数来迭代模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 ,loss:0.000075\n",
      "epoch 2 ,loss:0.000051\n",
      "epoch 3 ,loss:0.000067\n"
     ]
    }
   ],
   "source": [
    "num_epochs=3\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    for X,y in data_iter:\n",
    "        output=net1(X)\n",
    "        l=loss(output,y.view(-1,1))\n",
    "        optimizer.zero_grad() #梯度清零，等价于net.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    print('epoch %d ,loss:%f'%(epoch,l.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, -3] Parameter containing:\n",
      "tensor([[ 1.9993, -2.9998]], requires_grad=True)\n",
      "4.2 Parameter containing:\n",
      "tensor([4.2008], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "dense=net1[0]\n",
    "print(true_w,dense.weight)\n",
    "print(true_b,dense.bias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
