{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用重复元素的网络(VGG)\n",
    "\n",
    "VGG提出了可以通过重复使用简单的基础块来构建深度模型的思路。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG块\n",
    "\n",
    "VGG块的组成规律是：连续使用数个相同的填充为1、形状为3 * 3的卷积层后接上一个步幅为2、窗口形状为2 * 2的最大池化层。\n",
    "\n",
    "卷积层保持输入的高和宽不变，而池化层则对其减半。\n",
    "\n",
    "VGG通过小卷积核，保证在具有相同感知野的条件下，提升了网络的深度，在一定程度上提升了神经网络的效果。\n",
    "\n",
    "**每使用一个VGG块，形状会减半。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG网络\n",
    "\n",
    "VGG网络由卷积层模块后接全连接层模块构成。卷积层模块串联数个vgg_block,其超参数由变量conv_arch定义。该变量指定了每个VGG块里卷积层个数和输入输出通道数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 简单实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义VGG块\n",
    "def vgg_block(num_convs,in_channels,out_channels):\n",
    "    blk=[]\n",
    "    for i in range(num_convs):\n",
    "        if i==0:\n",
    "            blk.append(nn.Conv2d(in_channels,out_channels,kernel_size=3,padding=1))\n",
    "        else:\n",
    "            blk.append(nn.Conv2d(out_channels,out_channels,kernel_size=3,padding=1))\n",
    "    #加入池化层\n",
    "    blk.append(nn.MaxPool2d(kernel_size=2,stride=2)) #这里会使宽高减半\n",
    "    return nn.Sequential(*blk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义VGG网络\n",
    "def vgg(conv_arch,fc_features,fc_hidden_units=4096):\n",
    "    net=nn.Sequential()\n",
    "    #卷积层部分\n",
    "    for i,(num_convs,in_channels,out_channels) in enumerate(conv_arch):\n",
    "        #每经过一个vgg_block都会使高宽减半\n",
    "        net.add_module('vgg_block_'+str(i+1),vgg_block(num_convs,in_channels,out_channels))\n",
    "    #全连接层部分\n",
    "    net.add_module('fc',nn.Sequential(\n",
    "                        utils.FlattenLayer(),#首先要展开成向量\n",
    "                        nn.Linear(fc_features,fc_hidden_units),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(0.5),\n",
    "                        nn.Linear(fc_hidden_units,fc_hidden_units),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(0.5),\n",
    "                        nn.Linear(fc_hidden_units,10)\n",
    "                    ))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg_block_1 output shape: torch.Size([1, 64, 112, 112])\n",
      "vgg_block_2 output shape: torch.Size([1, 128, 56, 56])\n",
      "vgg_block_3 output shape: torch.Size([1, 256, 28, 28])\n",
      "vgg_block_4 output shape: torch.Size([1, 512, 14, 14])\n",
      "vgg_block_5 output shape: torch.Size([1, 512, 7, 7])\n",
      "fc output shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "#观察每一层的输出形状\n",
    "conv_arch=((1,1,64),(1,64,128),(2,128,256),(2,256,512),(2,512,512))\n",
    "#经过5个vgg_block,宽高减半5次，变成224/32=7\n",
    "fc_features=512*7*7\n",
    "fc_hidden_units=4096 #任意\n",
    "\n",
    "net=vgg(conv_arch,fc_features,fc_hidden_units)\n",
    "X=torch.rand(1,1,224,224)\n",
    "#named_children()获取一级子模块及其名字\n",
    "#named_modules()会返回所有子模块，包括子模块的子模块\n",
    "for name,blk in net.named_children():\n",
    "    X=blk(X)\n",
    "    print(name,'output shape:',X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (vgg_block_1): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_2): Sequential(\n",
      "    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_3): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_4): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_5): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): FlattenLayer()\n",
      "    (1): Linear(in_features=3136, out_features=512, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.5, inplace=False)\n",
      "    (7): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#获取数据及训练\n",
    "#为了简单化，这里我们使用更小通道数进行训练，节省时间\n",
    "ratio=8\n",
    "conv_arch=((1,1,64//ratio),(1,64//ratio,128//ratio),(2,128//ratio,256//ratio),(2,256//ratio,512//ratio),(2,512//ratio,512//ratio))\n",
    "net=vgg(conv_arch,fc_features//ratio,fc_hidden_units//ratio)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on cuda\n",
      "epoch 1,train loss 0.5094,train acc 0.8169,test acc 0.8763,time 64.3 sec\n",
      "epoch 2,train loss 0.3326,train acc 0.8803,test acc 0.8921,time 63.5 sec\n",
      "epoch 3,train loss 0.2892,train acc 0.8945,test acc 0.9007,time 63.2 sec\n",
      "epoch 4,train loss 0.2608,train acc 0.9054,test acc 0.9022,time 63.3 sec\n",
      "epoch 5,train loss 0.2408,train acc 0.9123,test acc 0.9050,time 62.6 sec\n"
     ]
    }
   ],
   "source": [
    "#训练\n",
    "batch_size=64\n",
    "train_iter,test_iter=utils.load_data_fashion_mnist(batch_size,resize=224)\n",
    "\n",
    "lr,num_epochs=0.001,5\n",
    "optimizer=torch.optim.Adam(net.parameters(),lr=lr)\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "utils.train_ch5(net,train_iter,test_iter,num_epochs,device,optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
